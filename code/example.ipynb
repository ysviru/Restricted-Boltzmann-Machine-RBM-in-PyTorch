{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiment can be a good method to verify if the code for the Bernoulli Restricted Boltzmann Machine (RBM) works correctly. I saw this idea in a talk by Prof. Hinton (https://www.youtube.com/watch?v=AyzOUbkUf3M, see from 12:55 onwards). \n",
    "\n",
    "In this case, we train the RBM on only the mnist digits with label as 2. We then test the RBM by supplying an image with a different label (something that it was not trained on, in this case, 3) and ask it to reconstruct back using n Gibbs sampling steps. If the RBM trained well to recognize 2s, it should convert the 3 to a 2. Note that since the RBM memorizes 2s, any other image is a confabulation and so the reconstruction process must try to recover the true memory (i.e., the images of digit 2) when supplied with the confabulation (i.e., the digit 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from BernoulliRBM import BernoulliRBM\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display digit \n",
    "def show_digit(x):\n",
    "    plt.imshow(x.reshape((28, 28)), cmap=plt.cm.gray)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,))\n",
    "train_data = train_loader.dataset.train_data\n",
    "train_labels = train_loader.dataset.train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter images for the digit 2. \n",
    "len_mnist_images = len(train_data)\n",
    "len_image = 784\n",
    "mnist_images_2 = []\n",
    "for i in range(len_mnist_images): \n",
    "    temp = train_labels[i]\n",
    "    if temp == 2:\n",
    "        mnist_images_2.append(train_data[i].type(torch.FloatTensor).resize_(len_image,1)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to one large matrix for rbm training. \n",
    "len_mnist_images_2 = len(mnist_images_2)\n",
    "mnist_image_matrix = torch.FloatTensor(len_image, len_mnist_images_2)\n",
    "for i in range(len_mnist_images_2): \n",
    "    mnist_image_matrix[:, i] = mnist_images_2[i].bernoulli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an example image of a different class, in this case, of digit 3.\n",
    "example_class = 3 \n",
    "mnist_test_image = torch.FloatTensor(len_image, 1)\n",
    "for i in range(len_mnist_images): \n",
    "    temp = train_labels[i]\n",
    "    if temp == example_class:\n",
    "        mnist_test_image = (train_data[i].type(torch.FloatTensor).resize_(len_image,1)/255).bernoulli()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting n_v =  784 , n_h =  500 , init_wt_var =  0.01 , lr =  0.01 , n_itr =  50 , bsz =  10 , verbose =  True , xv_init =  True , lr_decay =  True , inc_cd_k =  False , cdk =  5\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_vis = len(mnist_image_matrix[:, 0])\n",
    "n_hid = 500\n",
    "init_wt_var = 0.01\n",
    "l_rate = 0.01\n",
    "n_itr = 50\n",
    "bsz = 10\n",
    "verb = True\n",
    "xv_init = True\n",
    "lr_decay = True\n",
    "inc_cd_k = False\n",
    "cdk = 5\n",
    "\n",
    "print(\"Setting n_v = \", n_vis,\n",
    "      \", n_h = \", n_hid,\n",
    "      \", init_wt_var = \", init_wt_var,\n",
    "      \", lr = \", l_rate,\n",
    "      \", n_itr = \", n_itr,\n",
    "      \", bsz = \", bsz,\n",
    "      \", verbose = \", verb,\n",
    "      \", xv_init = \", xv_init,\n",
    "      \", lr_decay = \", lr_decay,\n",
    "      \", inc_cd_k = \", inc_cd_k,\n",
    "      \", cdk = \", cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rbm\n",
    "rbm = BernoulliRBM(n_vis,\n",
    "                   n_hid,\n",
    "                   init_weight_variance=init_wt_var,\n",
    "                   learning_rate=l_rate,\n",
    "                   n_epochs=n_itr,\n",
    "                   batch_size=bsz,\n",
    "                   verbose=verb,\n",
    "                   xavier_init=xv_init,\n",
    "                   learning_rate_decay=lr_decay,\n",
    "                   increase_to_cd_k=inc_cd_k,\n",
    "                   k=cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BernoulliRBM, fitting):   4%|4         | 26/596 [00:00<00:04, 122.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yogeshvirkar/Github/rbm/rbm/lib/python3.6/site-packages/torch/tensor.py:297: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.add_(other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 136.68it/s]\n",
      "epoch  1 , avg_cost =  62.04045783433338 , std_cost =  10.769680355068763 , avg_grad =  16156.153528405515 , std_grad =  4366.9376975637515 , time elapsed =  4.362382888793945\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 140.36it/s]\n",
      "epoch  2 , avg_cost =  51.085630288860145 , std_cost =  4.082860842176768 , avg_grad =  13505.208546888109 , std_grad =  672.4660576162144 , time elapsed =  4.248440980911255\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 139.97it/s]\n",
      "epoch  3 , avg_cost =  48.50140764889301 , std_cost =  3.873179595649698 , avg_grad =  13192.287432164954 , std_grad =  612.3063637240227 , time elapsed =  4.259500980377197\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 138.98it/s]\n",
      "epoch  4 , avg_cost =  47.09526979843242 , std_cost =  3.7437956621321793 , avg_grad =  12951.841537987626 , std_grad =  594.2479285859704 , time elapsed =  4.289978981018066\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 139.84it/s]\n",
      "epoch  5 , avg_cost =  46.21501907246225 , std_cost =  3.6846644697638684 , avg_grad =  12817.70151105023 , std_grad =  595.5563208650257 , time elapsed =  4.263794898986816\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 139.81it/s]\n",
      "epoch  6 , avg_cost =  45.59655988936456 , std_cost =  3.637644703591902 , avg_grad =  12731.391101811556 , std_grad =  586.9125922459313 , time elapsed =  4.265094041824341\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 139.31it/s]\n",
      "epoch  7 , avg_cost =  45.09758652936692 , std_cost =  3.678424964203448 , avg_grad =  12644.208476431419 , std_grad =  586.2154733543005 , time elapsed =  4.279834985733032\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 137.91it/s]\n",
      "epoch  8 , avg_cost =  44.59378812777116 , std_cost =  3.6002987373229396 , avg_grad =  12583.290304503986 , std_grad =  573.3840756723052 , time elapsed =  4.3231799602508545\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 138.38it/s]\n",
      "epoch  9 , avg_cost =  44.32606200403815 , std_cost =  3.583777869817549 , avg_grad =  12557.762275849413 , std_grad =  587.8380447436276 , time elapsed =  4.30872917175293\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 137.19it/s]\n",
      "epoch  10 , avg_cost =  44.19949536035525 , std_cost =  3.5870550711367875 , avg_grad =  12498.82956198878 , std_grad =  587.4076226009031 , time elapsed =  4.346019983291626\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 137.17it/s]\n",
      "epoch  11 , avg_cost =  43.913644022589565 , std_cost =  3.609814639972499 , avg_grad =  12469.127231674707 , std_grad =  580.2692144882241 , time elapsed =  4.34653902053833\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 137.35it/s]\n",
      "epoch  12 , avg_cost =  43.710269063911184 , std_cost =  3.60885289616281 , avg_grad =  12416.401365548973 , std_grad =  591.8538869590491 , time elapsed =  4.340982913970947\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 137.42it/s]\n",
      "epoch  13 , avg_cost =  43.58313189896961 , std_cost =  3.463885319156136 , avg_grad =  12403.00569716076 , std_grad =  561.0840055404647 , time elapsed =  4.338717222213745\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 136.55it/s]\n",
      "epoch  14 , avg_cost =  43.33761403384625 , std_cost =  3.5096857700678776 , avg_grad =  12366.16612868341 , std_grad =  572.6108241568202 , time elapsed =  4.366498708724976\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 136.12it/s]\n",
      "epoch  15 , avg_cost =  43.16340965552618 , std_cost =  3.47610540884505 , avg_grad =  12326.2848662306 , std_grad =  546.8564989427587 , time elapsed =  4.380373001098633\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 135.67it/s]\n",
      "epoch  16 , avg_cost =  43.24468370091995 , std_cost =  3.520666577956888 , avg_grad =  12330.165335636011 , std_grad =  573.4301243897463 , time elapsed =  4.394714117050171\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 135.86it/s]\n",
      "epoch  17 , avg_cost =  42.96096523976166 , std_cost =  3.5153550657348815 , avg_grad =  12266.592588283871 , std_grad =  569.9315607349153 , time elapsed =  4.388597011566162\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 135.53it/s]\n",
      "epoch  18 , avg_cost =  42.85212125714193 , std_cost =  3.5673376978790574 , avg_grad =  12255.977619350357 , std_grad =  581.64465488162 , time elapsed =  4.398810148239136\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 135.39it/s]\n",
      "epoch  19 , avg_cost =  42.82215321944064 , std_cost =  3.4264011601473263 , avg_grad =  12240.204407967178 , std_grad =  546.1567979126856 , time elapsed =  4.4042439460754395\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 134.32it/s]\n",
      "epoch  20 , avg_cost =  42.76753044768468 , std_cost =  3.568050715446401 , avg_grad =  12237.45553855128 , std_grad =  570.9803736942304 , time elapsed =  4.439013957977295\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 134.77it/s]\n",
      "epoch  21 , avg_cost =  42.648854063661304 , std_cost =  3.444575649816477 , avg_grad =  12215.930252792052 , std_grad =  569.1702616146877 , time elapsed =  4.424139022827148\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 133.08it/s]\n",
      "epoch  22 , avg_cost =  42.60645136737183 , std_cost =  3.5500772861135124 , avg_grad =  12198.112062185402 , std_grad =  543.7240486343626 , time elapsed =  4.480272054672241\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 129.42it/s]\n",
      "epoch  23 , avg_cost =  42.46748379572926 , std_cost =  3.467676184177889 , avg_grad =  12178.985407272441 , std_grad =  551.9982814995697 , time elapsed =  4.60783576965332\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 132.29it/s]\n",
      "epoch  24 , avg_cost =  42.407338891253374 , std_cost =  3.484435090777201 , avg_grad =  12171.35624705065 , std_grad =  563.3118551926124 , time elapsed =  4.5071351528167725\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 130.92it/s]\n",
      "epoch  25 , avg_cost =  42.35561930253202 , std_cost =  3.5137866852554738 , avg_grad =  12167.78725028838 , std_grad =  567.2794842361446 , time elapsed =  4.554011106491089\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 132.67it/s]\n",
      "epoch  26 , avg_cost =  42.252742447309046 , std_cost =  3.4585128269682364 , avg_grad =  12140.438810822148 , std_grad =  547.0695172475972 , time elapsed =  4.49470067024231\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 131.07it/s]\n",
      "epoch  27 , avg_cost =  42.25455261076857 , std_cost =  3.4530344253885095 , avg_grad =  12133.6519472263 , std_grad =  542.9881651412601 , time elapsed =  4.548892259597778\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 130.95it/s]\n",
      "epoch  28 , avg_cost =  42.19492841886994 , std_cost =  3.5094220029115135 , avg_grad =  12121.297664770344 , std_grad =  562.8874444917095 , time elapsed =  4.553714990615845\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 130.49it/s]\n",
      "epoch  29 , avg_cost =  42.01371431030683 , std_cost =  3.433543124293401 , avg_grad =  12099.51939033662 , std_grad =  563.5197449518835 , time elapsed =  4.570111036300659\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 129.69it/s]\n",
      "epoch  30 , avg_cost =  42.03615338370304 , std_cost =  3.508456086710091 , avg_grad =  12081.344053127621 , std_grad =  565.3692611167819 , time elapsed =  4.598317861557007\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 131.06it/s]\n",
      "epoch  31 , avg_cost =  42.10544789717501 , std_cost =  3.5604066993187327 , avg_grad =  12083.089152291317 , std_grad =  565.223467544503 , time elapsed =  4.549004793167114\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 130.39it/s]\n",
      "epoch  32 , avg_cost =  42.03548375072095 , std_cost =  3.528993247447399 , avg_grad =  12090.744194696414 , std_grad =  575.5135583370303 , time elapsed =  4.57301139831543\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 129.22it/s]\n",
      "epoch  33 , avg_cost =  41.79632093122341 , std_cost =  3.508433759442867 , avg_grad =  12055.98780116139 , std_grad =  550.2817889140974 , time elapsed =  4.6145689487457275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 140.75it/s]\n",
      "epoch  34 , avg_cost =  41.86089313430274 , std_cost =  3.4083227850797475 , avg_grad =  12040.304472852873 , std_grad =  548.1606028458759 , time elapsed =  4.236132860183716\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 140.41it/s]\n",
      "epoch  35 , avg_cost =  41.78172485300359 , std_cost =  3.4686550176251028 , avg_grad =  12054.372422595952 , std_grad =  559.1449554594528 , time elapsed =  4.246268033981323\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 140.72it/s]\n",
      "epoch  36 , avg_cost =  41.77291271030503 , std_cost =  3.451471145582472 , avg_grad =  12044.2913351379 , std_grad =  544.1846101033898 , time elapsed =  4.237365007400513\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 140.44it/s]\n",
      "epoch  37 , avg_cost =  41.734356259339606 , std_cost =  3.3917200011534336 , avg_grad =  12025.051421914324 , std_grad =  543.355325139339 , time elapsed =  4.245487928390503\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 133.67it/s]\n",
      "epoch  38 , avg_cost =  41.763684087151645 , std_cost =  3.4402160117277183 , avg_grad =  12021.241612376783 , std_grad =  547.1481821485119 , time elapsed =  4.4602952003479\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 138.44it/s]\n",
      "epoch  39 , avg_cost =  41.65737560451431 , std_cost =  3.4592947291554874 , avg_grad =  12013.6179510539 , std_grad =  556.5200994406061 , time elapsed =  4.306748867034912\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 135.61it/s]\n",
      "epoch  40 , avg_cost =  41.67470173547732 , std_cost =  3.4614482371049298 , avg_grad =  12002.975551528418 , std_grad =  548.4039662295179 , time elapsed =  4.39697790145874\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 136.10it/s]\n",
      "epoch  41 , avg_cost =  41.585395019326434 , std_cost =  3.496009088609255 , avg_grad =  12002.803289835885 , std_grad =  541.9041860618406 , time elapsed =  4.381105184555054\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 134.34it/s]\n",
      "epoch  42 , avg_cost =  41.616468077537995 , std_cost =  3.420891159867573 , avg_grad =  12006.643721817323 , std_grad =  541.7694911211466 , time elapsed =  4.438230037689209\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 134.54it/s]\n",
      "epoch  43 , avg_cost =  41.461845903588625 , std_cost =  3.349093269401342 , avg_grad =  11985.788731517408 , std_grad =  550.6520718147156 , time elapsed =  4.43158483505249\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 129.76it/s]\n",
      "epoch  44 , avg_cost =  41.50552510255135 , std_cost =  3.3525072935329416 , avg_grad =  11968.883346660024 , std_grad =  535.3853560528422 , time elapsed =  4.595590114593506\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:05<00:00, 107.57it/s]\n",
      "epoch  45 , avg_cost =  41.467918127175146 , std_cost =  3.4184725664679427 , avg_grad =  11981.563124279048 , std_grad =  548.766340819493 , time elapsed =  5.542756080627441\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 133.99it/s]\n",
      "epoch  46 , avg_cost =  41.36378351633981 , std_cost =  3.5000949366549006 , avg_grad =  11960.627480730915 , std_grad =  540.8303905748038 , time elapsed =  4.451316833496094\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 132.97it/s]\n",
      "epoch  47 , avg_cost =  41.36072992158416 , std_cost =  3.3651665344507538 , avg_grad =  11970.967329396497 , std_grad =  544.274929244265 , time elapsed =  4.485167980194092\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:05<00:00, 113.96it/s]\n",
      "epoch  48 , avg_cost =  41.342324436110935 , std_cost =  3.452886452382824 , avg_grad =  11957.07224432414 , std_grad =  524.8231043896488 , time elapsed =  5.232017993927002\n",
      "(BernoulliRBM, fitting): 100%|##########| 596/596 [00:04<00:00, 120.54it/s]\n",
      "epoch  49 , avg_cost =  41.32379231676959 , std_cost =  3.441890950828566 , avg_grad =  11954.465833420722 , std_grad =  538.8513505369123 , time elapsed =  4.946897029876709\n",
      "(BernoulliRBM, fitting):  19%|#8        | 113/596 [00:00<00:03, 121.00it/s]"
     ]
    }
   ],
   "source": [
    "# Fit rbm to training data. \n",
    "# The network understands only the digit 2. \n",
    "rbm.fit(mnist_image_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image of the learned class. \n",
    "plt.figure(1)\n",
    "image = mnist_image_matrix[:,10]\n",
    "show_digit(image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction of the sample image. \n",
    "plt.figure(2)\n",
    "_, image_reconst = rbm.reconstruct(image)\n",
    "show_digit(image_reconst.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weight matrix to visualize the feature detectors. \n",
    "W = rbm.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 25 feature detectors, i.e, the incoming weights to the first 25 hidden units. \n",
    "fig = plt.figure(3, figsize=(10,10))\n",
    "for i in range(25): \n",
    "    sub = fig.add_subplot(5, 5, i+1)\n",
    "    sub.imshow(W[i, :].numpy().reshape((28,28)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the test image of the different class (digit 3, in this case). \n",
    "show_digit(mnist_test_image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform reconstruction with rbm and display the reconstructed image\n",
    "# After 100 gibbs sampling steps, the rbm recalls a 2 since it was trained to memorize images of the digit 2. \n",
    "image = mnist_test_image\n",
    "_, image_reconst = rbm.reconstruct(image, n_gibbs=100)\n",
    "show_digit(image_reconst.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
